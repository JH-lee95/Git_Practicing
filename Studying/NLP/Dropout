Dropout 

1. 배치마다, 랜덤하게 특정 파라미터들을 0으로 만듦.
-> 학습 데이터의 패턴에 너무 익숙해지는 것을 방지함

2. 파라미터가 0이 되지 않은 남은 파라미터들이 더 많은 학습을 함.

3. 이전 배치에서 살아남은 파라미터가, 다음 배치에서 살아남을 파라미터들을 견제하는 역할을 함. 
(지나치게 학습하는 것을 막는 역할) 


4. 학습이 끝난 이후에는 모든 파라미터들을 다 살림

5. 50%를 드랍아웃 했다면, 최종 출력에서는 100% ( 모두 살리기 때문에) 가 됨. 이는 마치 모델을 두개를 돌린 효과를 냄. 즉 앙상블한 효과를 냄(voting)
