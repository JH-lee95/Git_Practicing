{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7 다층 퍼셉트론(MLP)으로 텍스트 분류하기\n",
    "이번에는 영문 뉴스 그룹의 토픽을 분류하는 예제를 자연어 전처리부터 딥러닝까지 한번에 실습하는 예제를 수행해보자.\n",
    "\n",
    "### 8.7.1 다층 퍼셉트론(MultiLayer Perceptron, MLP)\n",
    "hidden layer가 하나 이상 존재하는 신경망이다.\n",
    "\n",
    "### 8.7.2 keras의 texts_to_matrix()\n",
    "MLP로 텍스트 분류를 수행하기 전에 이번에 사용할 도구인 케라스 Tokenizer의 texts_to_matrix()를 이해해봅시다.\n",
    "\n",
    "먼저 Tokenizer로 학습데이터를 학습하여, 학습된 단어사전을 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'바나나': 1, '먹고': 2, '싶은': 3, '사과': 4, '길고': 5, '노란': 6, '저는': 7, '과일이': 8, '좋아요': 9}\n",
      "{1: '바나나', 2: '먹고', 3: '싶은', 4: '사과', 5: '길고', 6: '노란', 7: '저는', 8: '과일이', 9: '좋아요'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "texts = ['먹고 싶은 사과', '먹고 싶은 바나나', '길고 노란 바나나 바나나', '저는 과일이 좋아요']\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(texts)\n",
    "print(t.word_index)\n",
    "print(t.index_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 단어에 숫자 1부터 시작하는 정수 인덱스가 Corpus에서 노출된 빈도 순에 따라 부여되었다. \n",
    "\n",
    "이제 텍스트 데이터에 texts_to_matrix()를 사용하면, DTM를 생성해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 2. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(t.texts_to_matrix(texts, mode = 'count')) # texts_to_matrix의 입력으로 texts를 넣고, 모드는 'count'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t.texts_to_matrix 메소드의 mode 옵션은 다음과 같은 4개의 값을 입력할 수 있다.\n",
    "- \"binary\" : 각 doc에서 단어의 출현여부 만을 0, 1로 표시한 DTM를 생성. 기본값\n",
    "- \"count\" : 단어의 출현 빈도를 표시한 DTM 생성.\n",
    "- \"tfidf\" : 단어의 TF-IDF를 표시한 DTM 생성.\n",
    "- \"freq\" : 각 doc에서 단어의 출현 비율을 표시한 DTM 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]\n",
      "[[0.   0.   0.85 0.85 1.1  0.   0.   0.   0.   0.  ]\n",
      " [0.   0.85 0.85 0.85 0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   1.43 0.   0.   0.   1.1  1.1  0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   1.1  1.1  1.1 ]]\n",
      "[[0.   0.   0.33 0.33 0.33 0.   0.   0.   0.   0.  ]\n",
      " [0.   0.33 0.33 0.33 0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.5  0.   0.   0.   0.25 0.25 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.33 0.33 0.33]]\n"
     ]
    }
   ],
   "source": [
    "print(t.texts_to_matrix(texts, mode = 'binary'))\n",
    "print(t.texts_to_matrix(texts, mode = 'tfidf').round(2))\n",
    "print(t.texts_to_matrix(texts, mode = 'freq').round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7.3 20개 뉴스 그룹(Twenty Newsgroups) 데이터\n",
    "사이킷런에서는 20개의 다른 주제를 가진 18,846개의 뉴스 그룹 이메일 데이터를 제공합니다.\n",
    "\n",
    "훈련 샘플은 11,314개가 존재합니다. target_names에는 20개의 주제의 이름을 담고있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
      "훈련용 샘플의 개수 : 11314\n",
      "총 주제의 개수 : 20\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "newsdata = fetch_20newsgroups(subset = 'train') # 'train'을 기재하면 훈련 데이터만 리턴한다.\n",
    "print(newsdata.keys())\n",
    "print('훈련용 샘플의 개수 : {}'.format(len(newsdata.data)))\n",
    "print('총 주제의 개수 : {}'.format(len(newsdata.target_names)))\n",
    "print(newsdata.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n",
      "\n",
      "This module contains two loaders. The first one,\n",
      ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
      "returns a list of the raw texts that can be fed to text feature\n",
      "extractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`\n",
      "with custom parameters so as to extract feature vectors.\n",
      "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
      "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
      "extractor.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   ==========\n",
      "    Classes                     20\n",
      "    Samples total            18846\n",
      "    Dimensionality               1\n",
      "    Features                  text\n",
      "    =================   ==========\n",
      "\n",
      "Usage\n",
      "~~~~~\n",
      "\n",
      "The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
      "fetching / caching functions that downloads the data archive from\n",
      "the original `20 newsgroups website`_, extracts the archive contents\n",
      "in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
      ":func:`sklearn.datasets.load_files` on either the training or\n",
      "testing set folder, or both of them::\n",
      "\n",
      "  >>> from sklearn.datasets import fetch_20newsgroups\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
      "\n",
      "  >>> from pprint import pprint\n",
      "  >>> pprint(list(newsgroups_train.target_names))\n",
      "  ['alt.atheism',\n",
      "   'comp.graphics',\n",
      "   'comp.os.ms-windows.misc',\n",
      "   'comp.sys.ibm.pc.hardware',\n",
      "   'comp.sys.mac.hardware',\n",
      "   'comp.windows.x',\n",
      "   'misc.forsale',\n",
      "   'rec.autos',\n",
      "   'rec.motorcycles',\n",
      "   'rec.sport.baseball',\n",
      "   'rec.sport.hockey',\n",
      "   'sci.crypt',\n",
      "   'sci.electronics',\n",
      "   'sci.med',\n",
      "   'sci.space',\n",
      "   'soc.religion.christian',\n",
      "   'talk.politics.guns',\n",
      "   'talk.politics.mideast',\n",
      "   'talk.politics.misc',\n",
      "   'talk.religion.misc']\n",
      "\n",
      "The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
      "attribute is the integer index of the category::\n",
      "\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
      "\n",
      "It is possible to load only a sub-selection of the categories by passing the\n",
      "list of the categories to load to the\n",
      ":func:`sklearn.datasets.fetch_20newsgroups` function::\n",
      "\n",
      "  >>> cats = ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
      "\n",
      "  >>> list(newsgroups_train.target_names)\n",
      "  ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "\n",
      "Converting text to vectors\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "In order to feed predictive or clustering models with the text data,\n",
      "one first need to turn the text into vectors of numerical values suitable\n",
      "for statistical analysis. This can be achieved with the utilities of the\n",
      "``sklearn.feature_extraction.text`` as demonstrated in the following\n",
      "example that extract `TF-IDF`_ vectors of unigram tokens\n",
      "from a subset of 20news::\n",
      "\n",
      "  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "  >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
      "  ...               'comp.graphics', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectorizer = TfidfVectorizer()\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> vectors.shape\n",
      "  (2034, 34118)\n",
      "\n",
      "The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
      "components by sample in a more than 30000-dimensional space\n",
      "(less than .5% non-zero features)::\n",
      "\n",
      "  >>> vectors.nnz / float(vectors.shape[0])\n",
      "  159.01327...\n",
      "\n",
      ":func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which \n",
      "returns ready-to-use token counts features instead of file names.\n",
      "\n",
      ".. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n",
      ".. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n",
      "\n",
      "\n",
      "Filtering text for more realistic training\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "It is easy for a classifier to overfit on particular things that appear in the\n",
      "20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
      "high F-scores, but their results would not generalize to other documents that\n",
      "aren't from this window of time.\n",
      "\n",
      "For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
      "which is fast to train and achieves a decent F-score::\n",
      "\n",
      "  >>> from sklearn.naive_bayes import MultinomialNB\n",
      "  >>> from sklearn import metrics\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "  0.88213...\n",
      "\n",
      "(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
      "the training and test data, instead of segmenting by time, and in that case\n",
      "multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
      "yet of what's going on inside this classifier?)\n",
      "\n",
      "Let's take a look at what the most informative features are:\n",
      "\n",
      "  >>> import numpy as np\n",
      "  >>> def show_top10(classifier, vectorizer, categories):\n",
      "  ...     feature_names = np.asarray(vectorizer.get_feature_names())\n",
      "  ...     for i, category in enumerate(categories):\n",
      "  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
      "  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
      "  ...\n",
      "  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
      "  alt.atheism: edu it and in you that is of to the\n",
      "  comp.graphics: edu in graphics it is for and of to the\n",
      "  sci.space: edu it that is in and space to of the\n",
      "  talk.religion.misc: not it you in is that and to of the\n",
      "\n",
      "\n",
      "You can now see many things that these features have overfit to:\n",
      "\n",
      "- Almost every group is distinguished by whether headers such as\n",
      "  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
      "- Another significant feature involves whether the sender is affiliated with\n",
      "  a university, as indicated either by their headers or their signature.\n",
      "- The word \"article\" is a significant feature, based on how often people quote\n",
      "  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
      "  wrote:\"\n",
      "- Other features match the names and e-mail addresses of particular people who\n",
      "  were posting at the time.\n",
      "\n",
      "With such an abundance of clues that distinguish newsgroups, the classifiers\n",
      "barely have to identify topics from text at all, and they all perform at the\n",
      "same high level.\n",
      "\n",
      "For this reason, the functions that load 20 Newsgroups data provide a\n",
      "parameter called **remove**, telling it what kinds of information to strip out\n",
      "of each file. **remove** should be a tuple containing any subset of\n",
      "``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
      "blocks, and quotation blocks respectively.\n",
      "\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')\n",
      "  0.77310...\n",
      "\n",
      "This classifier lost over a lot of its F-score, just because we removed\n",
      "metadata that has little to do with topic classification.\n",
      "It loses even more if we also strip this metadata from the training data:\n",
      "\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "  0.76995...\n",
      "\n",
      "Some other classifiers cope better with this harder version of the task. Try\n",
      "running :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\n",
      "the ``--filter`` option to compare the results.\n",
      "\n",
      ".. topic:: Recommendation\n",
      "\n",
      "  When evaluating text classifiers on the 20 Newsgroups data, you\n",
      "  should strip newsgroup-related metadata. In scikit-learn, you can do this by\n",
      "  setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
      "  lower because it is more realistic.\n",
      "\n",
      ".. topic:: Examples\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsdata.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 챕터 실습의 목적은 테스트 데이터에서 이메일 본문을 보고 20개의 주제 중 어떤 주제인지를 맞추는 것입니다. 레이블인 target에는 총 0부터 19까지의 숫자가 들어가있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫번째 샘플의 레이블 : 7\n",
      "7번 레이블이 의미하는 주제 : rec.autos\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('첫번째 샘플의 레이블 : {}'.format(newsdata.target[0]))\n",
    "print('7번 레이블이 의미하는 주제 : {}'.format(newsdata.target_names[7]))\n",
    "print(newsdata.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: guykuo@carson.u.washington.edu (Guy Kuo)\n",
      "Subject: SI Clock Poll - Final Call\n",
      "Summary: Final call for SI clock reports\n",
      "Keywords: SI,acceleration,clock,upgrade\n",
      "Article-I.D.: shelley.1qvfo9INNc3s\n",
      "Organization: University of Washington\n",
      "Lines: 11\n",
      "NNTP-Posting-Host: carson.u.washington.edu\n",
      "\n",
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "\n",
      "Guy Kuo <guykuo@u.washington.edu>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsdata.data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이메일의 내용을 보니 스포츠 카에 대한 글로 보입니다. 이 글의 레이블은 7이고, 7번 레이블은 rec.autos란 주제를 의미합니다. 이제 훈련에 사용될 메일 본문인 data와 레이블인 target을 데이터프레임으로 만들어서 데이터에 대한 통계적인 정보들을 알아보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               email  target\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7\n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4\n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4\n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1\n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(newsdata.data, columns = ['email']) # data로부터 데이터프레임 생성\n",
    "data['target'] = pd.Series(newsdata.target) # target 열 추가\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11314 entries, 0 to 11313\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   email   11314 non-null  object\n",
      " 1   target  11314 non-null  int32 \n",
      "dtypes: int32(1), object(1)\n",
      "memory usage: 132.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2e124e82448>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['target'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 데이터프레임으로부터 다시 메일 본문과 레이블을 분리하고, 테스트 데이터 또한 불러오겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsdata_test = fetch_20newsgroups(subset='test', shuffle=True) # 'test'를 기재하면 테스트 데이터만 리턴한다.\n",
    "train_email = data['email'] # 훈련 데이터의 본문 저장\n",
    "train_label = data['target'] # 훈련 데이터의 레이블 저장\n",
    "test_email = newsdata_test.data # 테스트 데이터의 본문 저장\n",
    "test_label = newsdata_test.target # 테스트 데이터의 레이블 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 데이터와 테스트 데이터가 모두 준비되었습니다. 케라스의 토크나이저 도구를 사용하여 전처리를 진행해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000 # 실습에 사용할 단어의 최대 개수\n",
    "num_classes = 20 # 레이블의 수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 함수를 아래와 같이 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_data, test_data, mode): # 전처리 함수\n",
    "    t = Tokenizer(num_words = max_words) # max_words 개수만큼의 단어만 사용한다.\n",
    "    t.fit_on_texts(train_data)\n",
    "    X_train = t.texts_to_matrix(train_data, mode=mode) # 샘플 수 × max_words 크기의 행렬 생성\n",
    "    X_test = t.texts_to_matrix(test_data, mode=mode) # 샘플 수 × max_words 크기의 행렬 생성\n",
    "    return X_train, X_test, t.index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'binary' 모드를 사용하여 전처리 작업이 잘 수행되었는 지 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플 본문의 크기 : (11314, 10000)\n",
      "훈련 샘플 레이블의 크기 : (11314, 20)\n",
      "테스트 샘플 본문의 크기 : (7532, 10000)\n",
      "테스트 샘플 레이블의 크기 : (7532, 20)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, index_to_word = prepare_data(train_email, test_email, 'binary') # binary 모드로 변환\n",
    "y_train = to_categorical(train_label, num_classes) # 원-핫 인코딩\n",
    "y_test = to_categorical(test_label, num_classes) # 원-핫 인코딩\n",
    "\n",
    "print('훈련 샘플 본문의 크기 : {}'.format(X_train.shape))\n",
    "print('훈련 샘플 레이블의 크기 : {}'.format(y_train.shape))\n",
    "print('테스트 샘플 본문의 크기 : {}'.format(X_test.shape))\n",
    "print('테스트 샘플 레이블의 크기 : {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train은 전체 학습 메일에서 빈도수가 높은 1만개의 word에 대해 각, 메일에서의 출현여부를 저장하는 DTM이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,    2,    3,    4,    7,    8,    9,   11,   13,   14,   15,\n",
       "         16,   17,   18,   21,   25,   26,   27,   29,   32,   34,   35,\n",
       "         36,   38,   39,   42,   44,   47,   58,   63,   66,   77,   78,\n",
       "         80,   86,   88,   91,   95,  112,  123,  153,  171,  181,  182,\n",
       "        188,  198,  206,  211,  239,  250,  263,  273,  298,  337,  357,\n",
       "        384,  484,  610,  628,  708,  726,  808,  816,  828,  843,  844,\n",
       "       1078, 1162, 1308, 1341, 1347, 1350, 1376, 1497, 1498, 1767, 1835,\n",
       "       2266, 2804, 2908, 2952, 4017, 4308, 7860, 8164, 9879], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc0idx = np.where(X_train[0] == 1)[0]\n",
    "doc0idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'to', 'of', 'a', 'in', 'i', 'is', 'it', 'you', 'from', 'edu', 'on', 'this', 'be', 'have', 'or', 'was', 'if', 'subject', 'lines', 'organization', 'by', '2', 'my', 'can', 'what', 'all', 'there', 'your', 'me', 'out', 'university', 'posting', 'were', 'other', 'know', 'host', 'nntp', 'e', 'could', 'where', 'anyone', 'please', 'really', 'mail', 'years', 'thanks', '15', 'made', 'thing', 'day', 'name', 'car', 'called', 'info', 'looking', 'small', 'college', 'history', 'saw', 'body', 'whatever', 'rest', 'model', 'early', 'front', 'engine', 'looked', 'wondering', 'brought', 'late', 'umd', 'addition', 'door', 'il', 'park', 'separate', 'sports', 'production', 'specs', 'maryland', 'doors', 'wam', 'neighborhood', 'bumper', 'enlighten']\n"
     ]
    }
   ],
   "source": [
    "doc0words = []\n",
    "for idx in doc0idx:\n",
    "    doc0words.append(index_to_word[idx])\n",
    "\n",
    "print(doc0words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_email[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_words 옵션 값을 10000으로 지정하여, 이에 따른 shape을 얻었다. 이때 색인 0은 사용하지 않으므로, 실질적 상위 빈도수 9999개의 단어 행렬을 이룬다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "빈도수 상위 1번 단어 : the\n",
      "빈도수 상위 9999번 단어 : mic\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print('빈도수 상위 1번 단어 : {}'.format(index_to_word[1]))\n",
    "print('빈도수 상위 9999번 단어 : {}'.format(index_to_word[9999]))\n",
    "try:\n",
    "    print('사전에 없는 0번 단어 : {}'.format(index_to_word[0]))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불용어에 해당되는 단어 'the'가 빈도수 상위 1번 단어가 된 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7.4 MLP를 이용한 텍스트 분류\n",
    "[Quiz] 3개의 Dense layer와 중간 Dropout 레이어를 이용하여 adam 옵티마이저로 학습하고, test 데이터로 평가하는 fit_and_evaluate 함수를 작성한다.\n",
    "- 첫 2개의 Dense layer의 유닛수는 각각 256, 128로 설정한다.\n",
    "- 이때, 학습데이터의 10%를 평가용으로 split한다.\n",
    "- 학습이 종료된 후, test 데이터로 평가한 후, 평가결과를 리턴한다.\n",
    "\n",
    "prepare_data 함수와 fit_and_evaluate 함수를 이용하여 4가지 모드별로 평가를 진행하여, 어떤 모드가 더 성능이 좋은 지 알아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate(trainset, testset):\n",
    "    import keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_shape=(max_words,), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(*trainset, batch_size=128, epochs=5, verbose=1, validation_split=0.1)\n",
    "    score = model.evaluate(*testset, batch_size=128, verbose=0)\n",
    "    return score, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10182 samples, validate on 1132 samples\n",
      "Epoch 1/5\n",
      "10182/10182 [==============================] - 7s 693us/step - loss: 2.3012 - acc: 0.3311 - val_loss: 0.9869 - val_acc: 0.8083\n",
      "Epoch 2/5\n",
      "10182/10182 [==============================] - 4s 397us/step - loss: 0.8616 - acc: 0.7641 - val_loss: 0.4625 - val_acc: 0.8860\n",
      "Epoch 3/5\n",
      "10182/10182 [==============================] - 3s 286us/step - loss: 0.4387 - acc: 0.8802 - val_loss: 0.3470 - val_acc: 0.8966\n",
      "Epoch 4/5\n",
      "10182/10182 [==============================] - 5s 457us/step - loss: 0.2488 - acc: 0.9371 - val_loss: 0.2991 - val_acc: 0.9134\n",
      "Epoch 5/5\n",
      "10182/10182 [==============================] - 3s 253us/step - loss: 0.1687 - acc: 0.9602 - val_loss: 0.2847 - val_acc: 0.9178\n",
      "binary 모드의 테스트 손실/정확도: [0.6393319352218843, 0.8267392460741518]\n",
      "Train on 10182 samples, validate on 1132 samples\n",
      "Epoch 1/5\n",
      "10182/10182 [==============================] - 6s 582us/step - loss: 2.5542 - acc: 0.2870 - val_loss: 1.4077 - val_acc: 0.7553\n",
      "Epoch 2/5\n",
      "10182/10182 [==============================] - 4s 352us/step - loss: 1.2210 - acc: 0.6701 - val_loss: 0.6605 - val_acc: 0.8560\n",
      "Epoch 3/5\n",
      "10182/10182 [==============================] - 4s 388us/step - loss: 0.6641 - acc: 0.8322 - val_loss: 0.5342 - val_acc: 0.8772\n",
      "Epoch 4/5\n",
      "10182/10182 [==============================] - 4s 425us/step - loss: 0.4478 - acc: 0.8923 - val_loss: 0.4847 - val_acc: 0.8834\n",
      "Epoch 5/5\n",
      "10182/10182 [==============================] - 4s 345us/step - loss: 0.2951 - acc: 0.9329 - val_loss: 0.4405 - val_acc: 0.8922\n",
      "count 모드의 테스트 손실/정확도: [0.7106900347463007, 0.8215613381633465]\n",
      "Train on 10182 samples, validate on 1132 samples\n",
      "Epoch 1/5\n",
      "10182/10182 [==============================] - 8s 745us/step - loss: 2.2099 - acc: 0.3599 - val_loss: 0.7487 - val_acc: 0.8551\n",
      "Epoch 2/5\n",
      "10182/10182 [==============================] - 4s 356us/step - loss: 0.8080 - acc: 0.7760 - val_loss: 0.4145 - val_acc: 0.8913\n",
      "Epoch 3/5\n",
      "10182/10182 [==============================] - 4s 359us/step - loss: 0.4214 - acc: 0.8903 - val_loss: 0.3536 - val_acc: 0.9108\n",
      "Epoch 4/5\n",
      "10182/10182 [==============================] - 4s 385us/step - loss: 0.2842 - acc: 0.9257 - val_loss: 0.3268 - val_acc: 0.9090\n",
      "Epoch 5/5\n",
      "10182/10182 [==============================] - 4s 416us/step - loss: 0.1836 - acc: 0.9554 - val_loss: 0.3375 - val_acc: 0.9125\n",
      "tfidf 모드의 테스트 손실/정확도: [0.7021080272285088, 0.8327137548684187]\n",
      "Train on 10182 samples, validate on 1132 samples\n",
      "Epoch 1/5\n",
      "10182/10182 [==============================] - 7s 728us/step - loss: 2.9792 - acc: 0.0810 - val_loss: 2.9350 - val_acc: 0.1378\n",
      "Epoch 2/5\n",
      "10182/10182 [==============================] - 4s 367us/step - loss: 2.7506 - acc: 0.1870 - val_loss: 2.4615 - val_acc: 0.3860\n",
      "Epoch 3/5\n",
      "10182/10182 [==============================] - 4s 370us/step - loss: 2.2621 - acc: 0.3045 - val_loss: 1.9658 - val_acc: 0.5565\n",
      "Epoch 4/5\n",
      "10182/10182 [==============================] - 4s 401us/step - loss: 1.8029 - acc: 0.4606 - val_loss: 1.5086 - val_acc: 0.6643\n",
      "Epoch 5/5\n",
      "10182/10182 [==============================] - 4s 439us/step - loss: 1.4082 - acc: 0.5827 - val_loss: 1.1594 - val_acc: 0.7473.43 - ETA: 0s - loss: 1.4106 - acc: 0.58\n",
      "freq 모드의 테스트 손실/정확도: [1.3000228505658775, 0.6942379185005002]\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "models = {}\n",
    "preds = {}\n",
    "for mode in ['binary', 'count', 'tfidf', 'freq']:\n",
    "    X_train, X_test, _ = prepare_data(train_email, test_email, mode)\n",
    "    scores[mode], models[mode] = fit_and_evaluate((X_train, y_train), (X_test, y_test))\n",
    "    preds[mode] = models[mode].predict(X_test)\n",
    "    print(mode+' 모드의 테스트 손실/정확도:', scores[mode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'binary': [0.6393319352218843, 0.8267392460741518],\n",
       " 'count': [0.7106900347463007, 0.8215613381633465],\n",
       " 'tfidf': [0.7021080272285088, 0.8327137548684187],\n",
       " 'freq': [1.3000228505658775, 0.6942379185005002]}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성능이 좋지 않았던, freq 모델만 제외하고, 앙상블을 하여 최종 성능을 측정하면, 다음과 같은 결과를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  1,  0, ...,  9, 12, 15], dtype=int64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumprob = np.zeros((X_test.shape[0], num_classes))\n",
    "for mode in ['binary', 'count', 'tfidf']:\n",
    "    sumprob += preds[mode]\n",
    "    \n",
    "meanprob = sumprob / 3\n",
    "ensemblePred = np.argmax(meanprob, axis=1)\n",
    "ensemblePred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8454593733404142"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_label == ensemblePred).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로 서로 다른 로직의 모델을 경쟁시키는 앙상블 모델은 개별 모델들보다 항상 좋은 성능을 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
